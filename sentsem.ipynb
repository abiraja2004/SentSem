{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Konstantinos Oikonomou*\n",
    "\n",
    "Copyright (C) 2018. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentSem\n",
    "**Measurement of the Semantic Similarity Between Sentences**\n",
    "\n",
    "SentSem is a python script that takes any two English sentences as input and outputs the percentage of semantic similarity they have between them.\n",
    "\n",
    "The libraries used for the project are:\n",
    "- NLTK\n",
    "- numpy\n",
    "\n",
    "The Natural Language Toolkit for Python is used due to the fact that it provides access to almost every tool needed for the process. The main component needed is the *WordNet* corpus because of its structure and the similarity methods it provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up of the tokenizer, so that it keeps words and ignores any punctuation.\n",
    "\n",
    "Initialization of the Lemmatizer.\n",
    "\n",
    "Initialization of the `stopwords` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The POS function\n",
    "This function takes the results from the nltk.pos_tag() function and turns them into pos types that can be readable by WordNet. WordNet supports 5 different types, specifically Nouns, Verbs, Adverbs, Head Adjectives and Satelite Adjectives (Head and Satelite are too much about linguistics, we don't care about it very much). Here, we do not take into accouont the Satelite Adjectives but we will do so later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(tag):\n",
    "\n",
    "    if tag.startswith('N') or tag == 'MD':\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('RB'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LowerCase the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence:\n",
      "I love playing soccer!\n",
      "Second Sentence:\n",
      "I will play football.\n"
     ]
    }
   ],
   "source": [
    "sent1 = input('First Sentence:\\n').lower()\n",
    "sent2 = input('Second Sentence:\\n').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "We use the `RegexpTokenizer` we initiallized before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'playing', 'soccer']\n",
      "['i', 'will', 'play', 'football']\n"
     ]
    }
   ],
   "source": [
    "tokens1 = [word for word in tokenizer.tokenize(sent1)]\n",
    "tokens2 = [word for word in tokenizer.tokenize(sent2)]\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWord Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'playing', 'soccer']\n",
      "['play', 'football']\n"
     ]
    }
   ],
   "source": [
    "tokens1 = [word for word in tokens1 if word not in stopwords]\n",
    "tokens2 = [word for word in tokens2 if word not in stopwords]\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Tagging\n",
    "`nltk.pos_tag()` is used. Alternatively, other POS taggers could be more accurate in the classification of the parts of speech. It can be seen that this one is quite inaccurate, but it is used for example purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 'v'), ('playing', 'n'), ('soccer', 'n')]\n",
      "[('play', 'n'), ('football', 'n')]\n"
     ]
    }
   ],
   "source": [
    "text1 = [(w, pos(p)) for (w, p) in nltk.pos_tag(tokens1)]\n",
    "text2 = [(w, pos(p)) for (w, p) in nltk.pos_tag(tokens2)]\n",
    "\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Removal of suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 'v'), ('playing', 'n'), ('soccer', 'n')]\n",
      "[('play', 'n'), ('football', 'n')]\n"
     ]
    }
   ],
   "source": [
    "text1 = [(lemmatizer.lemmatize(word), p) for (word, p) in text1]\n",
    "text2 = [(lemmatizer.lemmatize(word), p) for (word, p) in text2]\n",
    "\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation (WSD)\n",
    "\n",
    "Words might be ambiguous, that is have multiple senses. For a word, WordNet creates a synset for each sense of the word. In order to find the correct synset (the correct word sense in our sentence), a common algorithm that is used is the Lesk algorithm (developed by Micheal Lesk).\n",
    "\n",
    "Here we also see that if the `lesk` algorithm returns None, there must be a problem with the POS tag that we have assigned through the `pos()` function. This specifically means that an *adjective* has been classified incorrectly as a head adjective, whereas it is a **satelite** adjective. That is why we use the if-statement for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', Synset('love.v.01')), ('playing', Synset('playing.n.02')), ('soccer', Synset('soccer.n.01'))]\n",
      "[('play', Synset('shimmer.n.01')), ('football', Synset('football.n.01'))]\n"
     ]
    }
   ],
   "source": [
    "sensed1 = [(word, lesk(sent1, word, p))\n",
    "           if (lesk(sent1, word, p) is not None) else (word, lesk(sent1, word, 's'))\n",
    "           for (word, p) in text1]\n",
    "sensed2 = [(word, lesk(sent2, word, p))\n",
    "           if (lesk(sent2, word, p) is not None) else (word, lesk(sent1, word, 's'))\n",
    "           for (word, p) in text2]\n",
    "\n",
    "print(sensed1)\n",
    "print(sensed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets1 = [syns for (_, syns) in sensed1]\n",
    "synsets2 = [syns for (_, syns) in sensed2]\n",
    "\n",
    "len1 = len(sensed1)\n",
    "len2 = len(sensed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Creation\n",
    "\n",
    "We create a matrix with as many rows as the remaining tokens of sentence 1 and as many columns as the remaining tokens of sentence 2. Let this matrix be A(i, j). Each element (i, j) of the matrix is assigned to be the path similarity between the ith token of sentence 1 and the jth token of sentence 2. Path similarity is a common similarity measure in WordNet, however another similarity metric might be more appropriate depending on the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.11111111,  0.08333334],\n",
      "       [ 0.14285715,  0.125     ],\n",
      "       [ 0.09090909,  0.5       ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sim_matrix = np.zeros((len1, len2), np.float32)\n",
    "for row in range(len1):\n",
    "    for col in range(len2):\n",
    "        \n",
    "        sim = synsets1[row].path_similarity(synsets2[col])\n",
    "\n",
    "        if sim is not None:\n",
    "            sim_matrix[row][col] = synsets1[row].path_similarity(synsets2[col])\n",
    "        else:\n",
    "            sim_matrix[row][col] = 0\n",
    "\n",
    "pprint(sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Pairs of Words\n",
    "\n",
    "Each word has a best pairing match, i.e. the word with which it presents maximum similarity.\n",
    "\n",
    "## Final Computation\n",
    "\n",
    "We add up the similarities of the pairs, multiply them by two and divide the result with the sum of the lengths of the two token lists. The reason to do this can be found in the following link: https://www.sciencedirect.com/science/article/pii/S1570866708000658 . This is an academic paper created by Fredriksson and Grabowski. It has some Graph Theory mathematics and robust supporting theory that explains the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.301587304473\n"
     ]
    }
   ],
   "source": [
    "sim_sum = max([sum(sim_matrix.max(axis=0)), sum(sim_matrix.max(axis=1))])\n",
    "print((2*sim_sum)/(len1+len2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
